{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input,Conv1D,MaxPooling1D,Dense,GlobalMaxPooling1D,Embedding,concatenate,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0           1                             2         3  \\\n",
       "0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...     ..         ...                           ...       ...   \n",
       "1599995  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                       4                                                  5  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('training.csv',encoding='ISO-8859-1',header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2, 3, 4, 5], dtype='int64')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=df.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  5\n",
       "0  0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1  0  is upset that he can't update his Facebook by ...\n",
       "2  0  @Kenichan I dived many times for the ball. Man...\n",
       "3  0    my whole body feels itchy and like its on fire \n",
       "4  0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop([1,2,3,4],axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text\n",
       "0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1      0  is upset that he can't update his Facebook by ...\n",
       "2      0  @Kenichan I dived many times for the ball. Man...\n",
       "3      0    my whole body feels itchy and like its on fire \n",
       "4      0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns=['Label','Text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label                                               Text\n",
       "0            0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1            0  is upset that he can't update his Facebook by ...\n",
       "2            0  @Kenichan I dived many times for the ball. Man...\n",
       "3            0    my whole body feels itchy and like its on fire \n",
       "4            0  @nationwideclass no, it's not behaving at all....\n",
       "...        ...                                                ...\n",
       "1599995      1  Just woke up. Having no school is the best fee...\n",
       "1599996      1  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997      1  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998      1  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999      1  happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].replace({4:1},inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Function to clean tweet text by removing links, special characters using simple regex statements. \n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\d+)|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Awww that s a bummer You shoulda got David Car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can t update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I dived many times for the ball Managed to sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>no it s not behaving at all i m mad why am i h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1</td>\n",
       "      <td>Just woke up Having no school is the best feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1</td>\n",
       "      <td>TheWDB com Very cool to hear old Walt interviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1</td>\n",
       "      <td>Are you ready for your MoJo Makeover Ask me fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1</td>\n",
       "      <td>Happy th Birthday to my boo of alll time Tupac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1</td>\n",
       "      <td>happy charitytuesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label                                               Text\n",
       "0            0  Awww that s a bummer You shoulda got David Car...\n",
       "1            0  is upset that he can t update his Facebook by ...\n",
       "2            0  I dived many times for the ball Managed to sav...\n",
       "3            0     my whole body feels itchy and like its on fire\n",
       "4            0  no it s not behaving at all i m mad why am i h...\n",
       "...        ...                                                ...\n",
       "1599995      1  Just woke up Having no school is the best feel...\n",
       "1599996      1   TheWDB com Very cool to hear old Walt interviews\n",
       "1599997      1  Are you ready for your MoJo Makeover Ask me fo...\n",
       "1599998      1  Happy th Birthday to my boo of alll time Tupac...\n",
       "1599999      1                               happy charitytuesday\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'] = df['Text'].apply(clean_tweet)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    800000\n",
       "0    800000\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens = [word_tokenize(sen) for sen in df.Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]    \n",
    "    \n",
    "lower_tokens = [lower_token(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "def removeStopWords(tokens): \n",
    "    return [word for word in tokens if word not in stoplist]\n",
    "filtered_words = [removeStopWords(sen) for sen in lower_tokens]\n",
    "df['Text_Final'] = [' '.join(sen) for sen in filtered_words]\n",
    "df['tokens'] = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stoplist = stopwords.words('english')\n",
    "# def removeStopWords(tokens): \n",
    "#     return [word for word in tokens if word not in stoplist]\n",
    "# filtered_words = [removeStopWords(sen) for sen in lower_tokens]\n",
    "df['Text_Final'] = [' '.join(sen) for sen in lower_tokens]\n",
    "df['tokens'] = lower_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_Final</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Label</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awww that s a bummer you shoulda got david car...</td>\n",
       "      <td>[awww, that, s, a, bummer, you, shoulda, got, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can t update his facebook by ...</td>\n",
       "      <td>[is, upset, that, he, can, t, update, his, fac...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "      <td>[i, dived, many, times, for, the, ball, manage...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no it s not behaving at all i m mad why am i h...</td>\n",
       "      <td>[no, it, s, not, behaving, at, all, i, m, mad,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Text_Final  \\\n",
       "0  awww that s a bummer you shoulda got david car...   \n",
       "1  is upset that he can t update his facebook by ...   \n",
       "2  i dived many times for the ball managed to sav...   \n",
       "3     my whole body feels itchy and like its on fire   \n",
       "4  no it s not behaving at all i m mad why am i h...   \n",
       "\n",
       "                                              tokens  Label  Pos  Neg  \n",
       "0  [awww, that, s, a, bummer, you, shoulda, got, ...      0    0    1  \n",
       "1  [is, upset, that, he, can, t, update, his, fac...      0    0    1  \n",
       "2  [i, dived, many, times, for, the, ball, manage...      0    0    1  \n",
       "3  [my, whole, body, feels, itchy, and, like, its...      0    0    1  \n",
       "4  [no, it, s, not, behaving, at, all, i, m, mad,...      0    0    1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "for l in df.Label:\n",
    "    if l == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif l == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "df['Pos']= pos\n",
    "df['Neg']= neg\n",
    "\n",
    "df = df[['Text_Final', 'tokens', 'Label', 'Pos', 'Neg']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(df,test_size=0.30,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14633234 words total, with a vocabulary size of 219539\n",
      "Max sentence length is 53\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6265886 words total, with a vocabulary size of 131935\n",
      "Max sentence length is 47\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "# Download link\n",
    "# https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'   \n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 219539 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(219540, 300)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1,EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "     train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    " \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, \n",
    "                        kernel_size=filter_size, \n",
    "                        activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['Pos', 'Neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 300)      65862000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 49, 200)      120200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 48, 200)      180200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 47, 200)      240200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 46, 200)      300200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 45, 200)      360200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 200)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 200)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1000)         0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1000)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          128128      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            258         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 67,191,386\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 65,862,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1,EMBEDDING_DIM, len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "24500/24500 [==============================] - 1674s 68ms/step - loss: 0.4768 - acc: 0.7711 - val_loss: 0.4177 - val_acc: 0.8085\n",
      "Epoch 2/3\n",
      "24500/24500 [==============================] - 1697s 69ms/step - loss: 0.4086 - acc: 0.8144 - val_loss: 0.4036 - val_acc: 0.8164\n",
      "Epoch 3/3\n",
      "24500/24500 [==============================] - 1716s 70ms/step - loss: 0.3854 - acc: 0.8265 - val_loss: 0.4064 - val_acc: 0.8175\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.3, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 219s 467ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8172125"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
    "labels = [1, 0]\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "sum(data_test.Label==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('depx007.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_tokenizer']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(tokenizer,'model_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9935799  0.00640973]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "text='I am happy'\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08902258 0.9109727 ]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text='I am not happy'\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01168683 0.9883045 ]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text='I am sad'\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01526377 0.98471594]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text='I am depressed'\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0281308  0.97185785]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text='I am not depressed'\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05708945 0.94290423]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text='I hate my life'\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37116742 0.62885225]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text=\"I disappear sometimes. It's my thing.\"\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0110409 0.9889561]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text=\"Sadness is to cry and to feel. But it’s that cold absence of feeling—that really hollowed-out feeling.\"\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.88131535 0.11873573]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "text=\"Success is not final; failure is not fatal: It is the courage to continue that counts\"\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9641641  0.03578565]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "text=\"Opportunities don't happen. You create them.\"\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.68796134 0.31208977]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "text=\"Stop chasing the money and start chasing the passion.\"\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73103523 0.2689943 ]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "text=\"The broken will always be able to love harder than most because once you’ve been in the dark, you learn to appreciate everything that shines.\"\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5780876  0.42195594]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "text=\"In addition to my other numerous acquaintances, I have one more intimate confidant… My depression is the most faithful mistress I have known—no wonder, then, that I return the love.\"\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19732514 0.80268157]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text=\"There are wounds that never show on the body that are deeper and more hurtful than anything that bleeds.\"\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3931939 0.6068242]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text=\"I can't eat and I can't sleep. I'm not doing well in terms of being a functional human, you know?\"\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8311243  0.16892958]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "text=\"Noble deeds and hot baths are the best cures for depression.\"\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30628777 0.6937268 ]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text=\"Noble deeds and hot baths are not the best cures for depression.\"\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.987365   0.01264215]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "text=\"I love my life\"\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9935799  0.00640973]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "text='I am happy'\n",
    "text = word_tokenize(text) \n",
    "text = lower_token(text)\n",
    "# text = removeStopWords(text)\n",
    "text = ' '.join(text)\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08902258 0.9109727 ]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text='I am not happy'\n",
    "text = word_tokenize(text) \n",
    "text = lower_token(text)\n",
    "# text = removeStopWords(text)\n",
    "text = ' '.join(text)\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43546697 0.5645627 ]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text=\"I can't eat and I can't sleep. I'm not doing well in terms of being a functional human, you know?\"\n",
    "text = word_tokenize(text) \n",
    "text = lower_token(text)\n",
    "# text = removeStopWords(text)\n",
    "text = ' '.join(text)\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02805674 0.9719398 ]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text=\"I am not sad\"\n",
    "\n",
    "predict=model.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input,Conv1D,MaxPooling1D,Dense,GlobalMaxPooling1D,Embedding,concatenate,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import seaborn as sns\n",
    "import re\n",
    "from tensorflow.keras.models import load_model\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmodel = load_model('depx007.h5')\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "tokenizer = load('model_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9935799  0.00640973]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "text='I am happy'\n",
    "\n",
    "predict=dmodel.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08902258 0.9109727 ]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text='I am not happy'\n",
    "\n",
    "predict=dmodel.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01526377 0.98471594]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text='I am depressed'\n",
    "\n",
    "predict=dmodel.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5780876  0.42195594]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "text=\"In addition to my other numerous acquaintances, I have one more intimate confidant… My depression is the most faithful mistress I have known—no wonder, then, that I return the love.\"\n",
    "\n",
    "predict=dmodel.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19732514 0.80268157]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text=\"There are wounds that never show on the body that are deeper and more hurtful than anything that bleeds.\"\n",
    "\n",
    "predict=dmodel.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30628777 0.6937268 ]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "text=\"Noble deeds and hot baths are not the best cures for depression.\"\n",
    "\n",
    "predict=dmodel.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8311243  0.16892958]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "text=\"Noble deeds and hot baths are the best cures for depression.\"\n",
    "\n",
    "predict=dmodel.predict(pad_sequences(tokenizer.texts_to_sequences([text]),maxlen=MAX_SEQUENCE_LENGTH))\n",
    "print(predict)\n",
    "print(np.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open( 'dict.json' , 'w' ) as file:\n",
    "    json.dump( tokenizer.word_index , file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
